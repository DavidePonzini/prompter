\section{Large Language Models e IA Generativa}
    % TODO

    \subsection{Large Language Models (LLM) e le loro principali caratteristiche}
        I LLM sono sistemi di IA che utilizzano l'architettura \textit{Transformer} per elaborare e generare linguaggio umano su vasta scala. Questi modelli, grazie ai milioni o miliardi di parametri e all'ampio training su diversi testi, sono capaci di comprendere e produrre testo in modo sorprendentemente umano.
    
        L'architettura transformer è stata introdotta nel 2017 da Google Research nel loro articolo ``Attention is All You Need''. Da allora, questa architettura è diventata la base per la maggior parte dei modelli avanzati di elaborazione del linguaggio naturale (NLP) grazie alla sua efficacia nell'elaborazione di sequenze di dati, come le parole in una frase, e per il suo approccio parallelo nell'elaborazione delle informazioni.
        
        Il Transformer si basa su due componenti principali:
        \begin{itemize}
            \item encoder, che analizza e codifica l'input linguistico in una forma comprensibile per il modello, e
            \item decoder, che utilizza queste informazioni codificate per generare output sequenziali.
        \end{itemize}
            
        Il cuore del Transformer è però il meccanismo di attenzione che permette al modello di valutare l'importanza relativa di ogni parola rispetto alle altre nella stessa frase, migliorando così la qualità della comprensione e della generazione del testo.
        
        Grazie a queste caratteristiche, i Transformer sono ideali per una varietà di applicazioni IA, come la traduzione automatica, la composizione di testi, il riassunto e la risposta a domande. Questi modelli non solo elaborano tutte le parole contemporaneamente, rendendo il processo più veloce, ma sono anche in grado di apprendere sottili sfumature linguistiche, il che li rende strumenti potenti e versatili nel campo dell'IA.

    \subsection{IA Generativa}
        % TODO

    \subsection{Strumenti e piattaforme per il Prompt Engineering}
    \subsection{Strumenti di IA Generativa disponibili oggi sul mercato}
        \subsubsection{Applicazioni pratiche}
    \subsection{Concetti base dell’architettura Transformer}
    \subsection{LLM basati su architettura Transformer}
        \subsubsection{LLM, architettura Transformer e machine learning}
        \subsubsection{Caratteristiche rilevanti dei LLM}
        \subsubsection{Come funzionano i LLM}
    \subsection{Come avviene il processo di interpretazione dei prompt}
    \subsection{Il concetto di ``embedding''}
    \subsection{``Punti di attenzione'' e ``fine-tuning''}
    \subsection{I principali parametri di configurazione di un LLM}
        \subsubsection{Temperatura}
        \subsubsection{Top p (nucleus sampling)}
        \subsubsection{Lunghezza massima}
        \subsubsection{Sequenze di stop}
        \subsubsection{Penalità di frequenza}
        \subsubsection{Penalità di presenza}
    \subsection{Esempio pratico di uso con GPT}
    \subsection{Esempio di interazione con le API GPT-3 in Python}
    \subsection{Personalizzare attraverso piattaforme web}
    \subsection{Personalizzare Chat-GPT}
    \subsection{Principi e tecniche di prompt engineering}
        \subsubsection{Come funzionano i prompt}
        \subsubsection{Interpretazione dei prompt}
        \subsubsection{Embedding}
        \subsubsection{Apprendimento non supervisionato e supervisionato}
    \subsection{Rappresentazione interna}
    \subsection{Rappresentazione del testo}
        \subsubsection{Punti di attenzione}
        \subsubsection{Fine tuning}
    \subsection{Risposta ai prompt}
        \subsubsection{Esempi}
    \subsection{Prompt impliciti e prompt espliciti}
        \subsubsection{Prompt impliciti}
        \subsubsection{Esempio di prompt implicito}
        \subsubsection{Prompt espliciti}
        \subsubsection{Esempio di prompt esplicito}
    \subsection{Prompt espliciti vs. promt impliciti: esempi}
    \subsection{Il fenomeno delle ``allucinazioni''}
    \subsection{Strumenti di IA Generativa attraverso API}
